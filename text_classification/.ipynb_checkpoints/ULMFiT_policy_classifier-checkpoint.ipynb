{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oKF4ml2z-UBB"
   },
   "outputs": [],
   "source": [
    "import fastai\n",
    "from fastai import *\n",
    "from fastai.text import * \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import io\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qHvcQBvzaCl"
   },
   "outputs": [],
   "source": [
    "# https://github.com/prateekjoshi565/ULMFiT_Text_Classification/blob/master/ULMFiT_fastai_Text_Classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26850,
     "status": "ok",
     "timestamp": 1582284904479,
     "user": {
      "displayName": "Lahiru Manohara",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCI-AJifHSePud6EDBiTvjaPYDpKOXY1zCIuEuCJg=s64",
      "userId": "03480165537579501658"
     },
     "user_tz": 0
    },
    "id": "U3ZRX804ALfu",
    "outputId": "b4f5d7c8-bca9-402e-9248-4e0c98ebd6b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u8JSCUgGAEJw"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(stop_words, tokens):\n",
    "    res = []\n",
    "    for token in tokens:\n",
    "        if not token in stop_words:\n",
    "            res.append(token)\n",
    "    return res\n",
    "\n",
    "def process_text(text):\n",
    "    text = text.encode('ascii', errors='ignore').decode()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = re.sub(r'#+', ' ', text )\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', ' ', text)\n",
    "    text = re.sub(r\"([A-Za-z]+)'s\", r\"\\1 is\", text)\n",
    "    #text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"won't\", \"will not \", text)\n",
    "    text = re.sub(r\"isn't\", \"is not \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemma_list = []\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token, 'v')\n",
    "        if lemma == token:\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "        lemma_list.append(lemma)\n",
    "    # return [ lemmatizer.lemmatize(token, 'v') for token in tokens ]\n",
    "    return lemma_list\n",
    "\n",
    "\n",
    "def process_all(text):\n",
    "    text = process_text(text)\n",
    "    return ' '.join(remove_stopwords(stop_words, text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 931,
     "status": "ok",
     "timestamp": 1582284948935,
     "user": {
      "displayName": "Lahiru Manohara",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCI-AJifHSePud6EDBiTvjaPYDpKOXY1zCIuEuCJg=s64",
      "userId": "03480165537579501658"
     },
     "user_tz": 0
    },
    "id": "-6KQ11KnAbA2",
    "outputId": "e0317e08-e278-4e3d-b9b3-2e6dbb0fcb7f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>segment_id</th>\n",
       "      <th>statement</th>\n",
       "      <th>slr</th>\n",
       "      <th>ner</th>\n",
       "      <th>slr_based_class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1050,15,10</td>\n",
       "      <td>You may also choose to provide us with additio...</td>\n",
       "      <td>{'verbs': [{'verb': 'may', 'description': 'You...</td>\n",
       "      <td>None</td>\n",
       "      <td>PUPR</td>\n",
       "      <td>you may also choose to provide us with additio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>1221,0,7</td>\n",
       "      <td>In some areas, however, you may choose service...</td>\n",
       "      <td>{'verbs': [{'verb': 'may', 'description': 'In ...</td>\n",
       "      <td>None</td>\n",
       "      <td>PUPR</td>\n",
       "      <td>in some areas however you may choose services ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>1618,24,11</td>\n",
       "      <td>You may not republish any content from the Sit...</td>\n",
       "      <td>{'verbs': [{'verb': 'may', 'description': 'You...</td>\n",
       "      <td>None</td>\n",
       "      <td>PUPR</td>\n",
       "      <td>you may not republish any content from the sit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>1099,15,1</td>\n",
       "      <td>HOW CAN YOU CONTROL YOUR PERSONAL INFORMATION?</td>\n",
       "      <td>{'verbs': [{'verb': 'CAN', 'description': 'HOW...</td>\n",
       "      <td>None</td>\n",
       "      <td>PUPR</td>\n",
       "      <td>how can you control your personal information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79</td>\n",
       "      <td>1468,18,10</td>\n",
       "      <td>You can clear flash cookies already on your de...</td>\n",
       "      <td>{'verbs': [{'verb': 'can', 'description': 'You...</td>\n",
       "      <td>None</td>\n",
       "      <td>PUPR</td>\n",
       "      <td>you can clear flash cookies already on your de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1838</td>\n",
       "      <td>93,7,3</td>\n",
       "      <td>We will never sell, rent, or swap your persona...</td>\n",
       "      <td>{'verbs': [{'verb': 'will', 'description': '[A...</td>\n",
       "      <td>None</td>\n",
       "      <td>POC</td>\n",
       "      <td>we will never sell rent or swap your personall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1885</td>\n",
       "      <td>1050,21,6</td>\n",
       "      <td>In doing so, we will request that you provide ...</td>\n",
       "      <td>{'verbs': [{'verb': 'doing', 'description': 'I...</td>\n",
       "      <td>None</td>\n",
       "      <td>POC</td>\n",
       "      <td>in doing so we will request that you provide y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>1909</td>\n",
       "      <td>481,5,3</td>\n",
       "      <td>Unless stated otherwise, we will use our best ...</td>\n",
       "      <td>{'verbs': [{'verb': 'stated', 'description': '...</td>\n",
       "      <td>Post</td>\n",
       "      <td>POC</td>\n",
       "      <td>unless stated otherwise we will use our best e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>1980</td>\n",
       "      <td>1708,24,2</td>\n",
       "      <td>We will retain your information for the period...</td>\n",
       "      <td>{'verbs': [{'verb': 'will', 'description': 'We...</td>\n",
       "      <td>None</td>\n",
       "      <td>POC</td>\n",
       "      <td>we will retain your information for the period...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>1986</td>\n",
       "      <td>1259,9,2</td>\n",
       "      <td>We will not provide the third-party service pr...</td>\n",
       "      <td>{'verbs': [{'verb': 'will', 'description': 'We...</td>\n",
       "      <td>None</td>\n",
       "      <td>POC</td>\n",
       "      <td>we will not provide the third party service pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>572 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  ...                                               text\n",
       "0            16  ...  you may also choose to provide us with additio...\n",
       "1            32  ...  in some areas however you may choose services ...\n",
       "2            51  ...  you may not republish any content from the sit...\n",
       "3            52  ...      how can you control your personal information\n",
       "4            79  ...  you can clear flash cookies already on your de...\n",
       "..          ...  ...                                                ...\n",
       "567        1838  ...  we will never sell rent or swap your personall...\n",
       "568        1885  ...  in doing so we will request that you provide y...\n",
       "569        1909  ...  unless stated otherwise we will use our best e...\n",
       "570        1980  ...  we will retain your information for the period...\n",
       "571        1986  ...  we will not provide the third party service pr...\n",
       "\n",
       "[572 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/content/drive/My Drive/research_project/nlp_privacy_policy_analyze/data/processed_privacy_policy_segments_sample_2000_1_class.csv', encoding='utf-8')\n",
    "# filtredData = df[df['annotated_class'] != 'OTHER']\n",
    "filtredData = df\n",
    "for i, row in filtredData.iterrows():\n",
    "  filtredData.at[i, 'text'] = process_text(row['statement'])\n",
    "\n",
    "filtredData.reset_index(drop=True, inplace=True)\n",
    "\n",
    "filtredData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 871,
     "status": "ok",
     "timestamp": 1582285025048,
     "user": {
      "displayName": "Lahiru Manohara",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCI-AJifHSePud6EDBiTvjaPYDpKOXY1zCIuEuCJg=s64",
      "userId": "03480165537579501658"
     },
     "user_tz": 0
    },
    "id": "GwYnlIVfBStC",
    "outputId": "1f748140-65b7-4b7e-f3ea-9776909cfe97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((343, 2), (229, 2))"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(filtredData['slr_based_class'])\n",
    "sentence = list(filtredData['text'])\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "list(le.classes_)\n",
    "lables_enc = le.transform(labels)\n",
    "filtredData['label'] = lables_enc\n",
    "\n",
    "df_trn, df_val = train_test_split(filtredData[['label', 'text']], stratify = filtredData['label'], test_size = 0.4, random_state = 12)\n",
    "\n",
    "df_trn.shape, df_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42004,
     "status": "ok",
     "timestamp": 1582289725488,
     "user": {
      "displayName": "Lahiru Manohara",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCI-AJifHSePud6EDBiTvjaPYDpKOXY1zCIuEuCJg=s64",
      "userId": "03480165537579501658"
     },
     "user_tz": 0
    },
    "id": "1FAWYCKBGeMS",
    "outputId": "d8f5ebcb-93d5-4865-d4c3-c556f592e0a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.158353</td>\n",
       "      <td>4.594241</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Language model data\n",
    "data_lm = TextLMDataBunch.from_df(train_df = df_trn, valid_df = df_val, path = \"\")\n",
    "\n",
    "# Classifier model data\n",
    "data_clas = TextClasDataBunch.from_df(path = \"\", train_df = df_trn, valid_df = df_val, vocab=data_lm.train_ds.vocab, bs=32)\n",
    "\n",
    "learn = language_model_learner(data_lm,  arch = AWD_LSTM, drop_mult=0.7)\n",
    "\n",
    "# train the learner object with learning rate = 1e-2\n",
    "learn.fit_one_cycle(1, 1e-1)\n",
    "\n",
    "# learn.lr_find()\n",
    "\n",
    "# learn.recorder.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 74806,
     "status": "ok",
     "timestamp": 1582291390142,
     "user": {
      "displayName": "Lahiru Manohara",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCI-AJifHSePud6EDBiTvjaPYDpKOXY1zCIuEuCJg=s64",
      "userId": "03480165537579501658"
     },
     "user_tz": 0
    },
    "id": "wrTCRx9ZOHhd",
    "outputId": "11257609-e5da-403e-8cc8-3c91a3d91ece"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_beta</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.252567</td>\n",
       "      <td>1.092158</td>\n",
       "      <td>0.541485</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.277253</td>\n",
       "      <td>0.248545</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.save_encoder('ft_enc')\n",
    "\n",
    "learn = text_classifier_learner(data_clas, arch = AWD_LSTM, drop_mult=0.7)\n",
    "learn.load_encoder('ft_enc')\n",
    "\n",
    "# learn.lr_find()\n",
    "# learn.recorder.plot()\n",
    "\n",
    "learn.metrics = [accuracy, Precision(average='macro'), Recall(average='macro'), FBeta(average='macro') ]\n",
    "learn.fit_one_cycle(1, 1e-1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP5gZQitytKE6SWYUhAI5BU",
   "collapsed_sections": [],
   "name": "ULMFiT_policy_classifier.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
