{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import io\n",
    "import string\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import en_core_web_sm\n",
    "from ast import literal_eval\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "import logging\n",
    "from pprint import pprint\n",
    "from nltk.corpus import framenet \n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsedPolicySegments(policyFile):\n",
    "    policyData = []\n",
    "    with open(policyFile) as f:\n",
    "        policySegments = json.load(f)\n",
    "    for key,value in policySegments.items():\n",
    "        wordCount = len(value.split())\n",
    "        match = re.findall(r'(<[^>]*>)', value)\n",
    "        if not match and wordCount >= 3:\n",
    "            policyData.append([key,value])\n",
    "    \n",
    "    df = pd.DataFrame(policyData, columns = ['segment_id','statement'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectPolicySentences(segmentDf, selections):\n",
    "    selectedPolicies = []\n",
    "    for i, row in segmentDf.iterrows():\n",
    "        segmentId = row['segment_id'] \n",
    "        statement = row['statement']\n",
    "        if int(segmentId.split(\",\", 1)[0]) in selections:\n",
    "            selectedPolicies.append([segmentId, statement])\n",
    "    \n",
    "    df = pd.DataFrame(selectedPolicies, columns = ['segment_id','statement'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsedAppPolicyData(filePath):\n",
    "    tokenizedSatatements = []\n",
    "    for filename in os.listdir(filePath):\n",
    "        with open(filePath+filename) as f:\n",
    "            for sentence in f:\n",
    "                tokens = preprocess_string(sentence)\n",
    "                tokenizedSatatements.append(tokens)\n",
    "    return tokenizedSatatements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleanSegments(segmentDf):\n",
    "#     for i, row in segmentDf.iterrows():\n",
    "#         url_free = re.sub(r'(<[^>]*>.*?<[^>]*>)', '', row['statement'], flags=re.MULTILINE)\n",
    "#         segmentDf.at[i,'statement'] = url_free\n",
    "#     return segmentDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessedSegments(segmentDf):\n",
    "    tokenizedSatatements = []\n",
    "    for i, row in segmentDf.iterrows():\n",
    "        tokens = preprocess_string(row['statement'])\n",
    "        tokenizedSatatements.append(tokens)\n",
    "    return tokenizedSatatements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWordEmbedding(tokenizedSatatements):\n",
    "    model = Word2Vec(tokenizedSatatements, size=100, window=5, min_count=5, workers=4,sg=0);\n",
    "    model.save('word_embedding_model/word2vec_app_policy_modal.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFastTextModel(filePath):\n",
    "    model = Word2Vec.load(filePath)\n",
    "    print(model.wv.most_similar(\"use\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGoogleDataModel(filePath):\n",
    "    model = KeyedVectors.load_word2vec_format(filePath, binary=True)\n",
    "    print(model.most_similar('use'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extractor(segmentDf):\n",
    "# #     slrPredictor = Predictor.from_path(\"/home/lahiru/Research/policy_analysis/src/scripts/commitment_analysis/bert-base-srl-2019.06.17.tar.gz\")\n",
    "# #     nerPredictor = predictor = Predictor.from_path(\"/home/lahiru/Research/policy_analysis/src/scripts/commitment_analysis/ner-model-2018.12.18.tar.gz\")\n",
    "# #     depandencyPredictor = Predictor.from_path(\"/home/lahiru/Research/policy_analysis/src/scripts/commitment_analysis/biaffine-dependency-parser-ptb-2018.08.23.tar.gz\")\n",
    "#     j = 0;\n",
    "#     for i, row in segmentDf.iterrows():\n",
    "#         print(j)\n",
    "#         sentence = row['statement']\n",
    "#         print(sentence)\n",
    "#         if(pd.notna(sentence) and sentence.strip() != \"\"):\n",
    "#             if(pd.notna(row['slr_relation'])):\n",
    "#                 segmentDf.at[i,'slr_based_class'] = str(annotatedBySRL(row['slr'], row['ner'])\n",
    "# #             segmentDf.at[i,'slr'] = extractSLR(sentence, slrPredictor)\n",
    "# #             segmentDf.at[i,'actor'] = extractActor(sentence)\n",
    "# #             segmentDf.at[i,'scope'] = extractScope(sentence)\n",
    "# #             dependancyTree = extractDependancy(sentence, depandencyPredictor)\n",
    "# #             segmentDf.at[i,'dependancy'] = str(dependancyTree)\n",
    "# #             segmentDf.at[i,'ner'] = str(nameEntityRecognition(sentence, nerPredictor))\n",
    "# #             result = {}\n",
    "# #             result['children'] = []\n",
    "# #             segmentDf.at[i,'concept'] = extractConcept(literal_eval(row['dependancy']), result)\n",
    "# #             segmentDf.at[i,'slr_relation'] = str(extractSLRRelation(row['slr'], row['dependancy']))\n",
    "# #             if(pd.notna(row['slr_relation'])):\n",
    "# #                segmentDf.at[i,'class'] = str(annotate(row['slr_relation']))\n",
    "# #                 arributes = extractAttributes(row['slr_relation'])\n",
    "# #                 if arributes is not None:\n",
    "# #                     segmentDf.at[i,'attributes'] = str(arributes)\n",
    "# #                     segmentDf.at[i,'actor'] = str(arributes['actor']) \n",
    "# #                     segmentDf.at[i,'action'] = str(arributes['action'])\n",
    "# #                     segmentDf.at[i,'object'] = str(arributes['object']) \n",
    "# #                     segmentDf.at[i,'object_source'] = str(arributes['object_source'])\n",
    "# #                     segmentDf.at[i,'target'] = str(arributes['target'])\n",
    "# #                     segmentDf.at[i,'purpose'] = str(arributes['purpose'])        \n",
    "# #             if(pd.notna(row['action'])):\n",
    "# #                 segmentDf.at[i,'lex_name'] = str(getLexName(row['action']))\n",
    "#         j+=1\n",
    "#     return segmentDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(segmentDf):\n",
    "#     slrPredictor = Predictor.from_path(\"/home/lahiru/Research/policy_analysis/src/scripts/commitment_analysis/bert-base-srl-2019.06.17.tar.gz\")\n",
    "#     nerPredictor = predictor = Predictor.from_path(\"/home/lahiru/Research/policy_analysis/src/scripts/commitment_analysis/ner-model-2018.12.18.tar.gz\")\n",
    "#     depandencyPredictor = Predictor.from_path(\"/home/lahiru/Research/policy_analysis/src/scripts/commitment_analysis/biaffine-dependency-parser-ptb-2018.08.23.tar.gz\")\n",
    "\n",
    "    j=0\n",
    "    for i, row in segmentDf.iterrows():\n",
    "        print(j)\n",
    "        sentence = row['statement']\n",
    "        print(sentence)\n",
    "        if(pd.notna(sentence) and sentence.strip() != \"\"):\n",
    "            \n",
    "            segmentDf.at[i,'slr'] = extractSLR(sentence, slrPredictor)\n",
    "            \n",
    "            dependancyTree = extractDependancy(sentence, depandencyPredictor)\n",
    "            segmentDf.at[i,'dependancy'] = str(dependancyTree)\n",
    "            segmentDf.at[i,'ner'] = str(nameEntityRecognition(sentence, nerPredictor))\n",
    "\n",
    "#             segmentDf.at[i,'slr_relation'] = str(extractSLRRelation(sentence, row['slr'], row['dependancy']))\n",
    "\n",
    "#             if(pd.notna(row['slr_relation'])):\n",
    "#                 segmentDf.at[i,'slr_based_class'] = str(annotatedBySRL(row['slr'], row['ner']))\n",
    "\n",
    "#             if(pd.notna(row['slr_relation'])):\n",
    "#                 arributes = extractAttributes(row['slr_relation'])\n",
    "#                 if arributes is not None:\n",
    "#                     segmentDf.at[i,'attributes'] = str(arributes)\n",
    "#                     segmentDf.at[i,'actor'] = str(arributes['actor']) \n",
    "#                     segmentDf.at[i,'action'] = str(arributes['action'])\n",
    "#                     segmentDf.at[i,'object'] = str(arributes['object']) \n",
    "#                     segmentDf.at[i,'object_source'] = str(arributes['object_source'])\n",
    "#                     segmentDf.at[i,'target'] = str(arributes['target'])\n",
    "#                     segmentDf.at[i,'purpose'] = str(arributes['purpose'])\n",
    "#                     segmentDf.at[i,'conditions'] = str(arributes['conditions'])\n",
    "                    \n",
    "#             if(pd.notna(row['action'])):\n",
    "#                 segmentDf.at[i,'lex_name'] = str(getLexName(row['action']))\n",
    "        j+=1\n",
    "    return segmentDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractActor(sentence):\n",
    "    nlp = en_core_web_sm.load();\n",
    "    pronoun = [x.orth_ for x in [y for y in nlp(sentence) if y.pos_ == 'PRON']]\n",
    "    duplicateRemoved = list(set(pronoun))\n",
    "    return str(duplicateRemoved)\n",
    "\n",
    "def extractSLR(sentence, predictor):\n",
    "    try:\n",
    "        return str(predictor.predict(sentence))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def extractScope(sentence):\n",
    "    return str(re.findall(r'\\b(?:law|regulation)\\b', sentence))\n",
    "\n",
    "def extractDependancy(sentence, predictor):\n",
    "    return predictor.predict(sentence)['hierplane_tree']['root']\n",
    "\n",
    "'''Extract concept from the dependacy parsing'''\n",
    "def extractConcept(dependancyTree, result):\n",
    "    #check root lement exist in the tree\n",
    "    if 'root' in dependancyTree.values():\n",
    "        result['root'] = dependancyTree['word']\n",
    "        \n",
    "    #check if children element exist in the tree\n",
    "    if 'children' in dependancyTree:\n",
    "        childrens = dependancyTree['children']\n",
    "        for children in childrens:\n",
    "            #check whether the nodetype is nsoubj or aux\n",
    "            if children['nodeType'] == 'nsubj' or children['nodeType'] == 'aux' or children['nodeType'] == 'poss':\n",
    "                result['children'].append({children['nodeType']:children['word']})\n",
    "            #if node has another children recursively call the funtions \n",
    "            if 'children' in children:\n",
    "                newChild = {'children':children['children']}\n",
    "                extractConcept(newChild, result)\n",
    "                \n",
    "    return str(result)\n",
    "\n",
    "def extractSLRRelation(sentence,slr,depandancy):\n",
    "    depandancyDic = literal_eval(depandancy)\n",
    "    slrDic = literal_eval(slr)\n",
    "    count = 0\n",
    "    verbPosition = 0\n",
    "    if 'word' in depandancyDic:\n",
    "        spans = depandancyDic['spans']\n",
    "        end = spans[0]['end']\n",
    "        sentenceSpan = sentence[:end]\n",
    "        words = sentenceSpan.split()\n",
    "        rootWord = depandancyDic['word']\n",
    "        for word in words:\n",
    "            if word.strip() == rootWord:\n",
    "                count += 1\n",
    "        for verb in slrDic['verbs']:\n",
    "            if verb['verb'] == rootWord:\n",
    "                verbPosition += 1\n",
    "            if verb['verb'] == rootWord and count <= 1:\n",
    "                return verb\n",
    "            elif verb['verb'] == rootWord and verbPosition == count:\n",
    "                print(verb)\n",
    "                return verb\n",
    "\n",
    "# def extractPUC(slr):\n",
    "#     pronoun = ['you','your']\n",
    "#     healpingVerb = ['must']\n",
    "            \n",
    "# def extractPUP(slr):\n",
    "\n",
    "# def extractPOC(slr):\n",
    "    \n",
    "# def extractPOP(slr):\n",
    "\n",
    "def nameEntityRecognition(sentence, predictor):\n",
    "    try:\n",
    "        prediction = predictor.predict(sentence)\n",
    "        tags = prediction['tags']            \n",
    "        words = prediction['words']\n",
    "    \n",
    "        for i, tag in enumerate(tags):\n",
    "            if tag == 'U-ORG':\n",
    "                return words[i];\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def annotate(slr):\n",
    "    slrDic = literal_eval(slr)\n",
    "    if 'verb' in slrDic:\n",
    "        description = slrDic['description']\n",
    "        entities = re.findall(r'([a-zA-Z0-9- ]*:[a-zA-Z0-9\\- ]*)', description)\n",
    "        print(entities)\n",
    "        dic = dict(s.split(':') for s in entities)\n",
    "        \n",
    "        arg0 = \"\"\n",
    "        mod = \"\"\n",
    "        verb = \"\"\n",
    "        if dic.get('ARG0') is not None:\n",
    "            arg0 = dic.get('ARG0').strip().lower()\n",
    "        if dic.get('ARGM-MOD') is not None:\n",
    "            mod = dic.get('ARGM-MOD').strip().lower()\n",
    "        if dic.get('V') is not None:\n",
    "            verb = dic.get('V').strip().lower()\n",
    "        \n",
    "        if arg0 in ['you','your'] and mod in ['must', 'will', 'would']:\n",
    "            return 'PUC'\n",
    "        elif arg0 in ['you', 'your'] and mod in ['may','might','can','could']:\n",
    "            return 'PUPR'\n",
    "#         elif arg0 in ['you', 'your'] and verb is not None:\n",
    "#             return 'PUPR'\n",
    "        elif arg0 in ['we', 'us', 'our'] and mod in ['must', 'will', 'would']:\n",
    "            return 'POC'\n",
    "#         elif arg0 in ['we', 'us', 'our'] and verb is not None:\n",
    "#             return 'POC'\n",
    "        elif arg0 in ['we', 'us', 'our'] and mod in ['may','might','can','could']:\n",
    "            return 'POPR'\n",
    "        \n",
    "        if arg0 in ['you','your'] and verb in ['agree', 'concent']:\n",
    "            return 'PUC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotatedBySRL(srl, ner):\n",
    "    slrDic = literal_eval(srl)\n",
    "    categories = []\n",
    "    org = ner.strip().lower()\n",
    "    if 'verbs' in slrDic:\n",
    "        verbs = slrDic['verbs']\n",
    "        for verb in verbs:\n",
    "            description = verb['description']\n",
    "            entities = re.findall(r'([a-zA-Z0-9- ]*:[a-zA-Z0-9\\- ]*)', description)\n",
    "            dic = dict(s.split(':') for s in entities)\n",
    "            \n",
    "            arg0 = \"\"\n",
    "            mod = \"\"\n",
    "            verb = \"\"\n",
    "            if dic.get('ARG0') is not None:\n",
    "                arg0 = dic.get('ARG0').strip().lower()\n",
    "            if dic.get('ARGM-MOD') is not None:\n",
    "                mod = dic.get('ARGM-MOD').strip().lower()\n",
    "            if dic.get('V') is not None:\n",
    "                verb = dic.get('V').strip().lower()\n",
    "\n",
    "            if arg0 in ['you','your'] and mod in ['must', 'will', 'would']:\n",
    "                categories.append('PUC')\n",
    "            elif arg0 in ['you', 'your'] and mod in ['may','might','can','could']:\n",
    "                categories.append('PUPR')\n",
    "    #         elif arg0 in ['you', 'your'] and verb is not None:\n",
    "    #             return 'PUPR'\n",
    "            elif arg0 in ['we', 'us', 'our'] and mod in ['must', 'will', 'would']:\n",
    "                categories.append('POC')\n",
    "    #         elif arg0 in ['we', 'us', 'our'] and verb is not None:\n",
    "    #             return 'POC'\n",
    "            elif arg0 in ['we', 'us', 'our'] and mod in ['may','might','can','could']:\n",
    "                categories.append('POPR')\n",
    "\n",
    "            if arg0 in ['you','your'] and verb in ['agree', 'concent', 'permit', 'required', 'submit', 'register', 'agreement', 'disclose']:\n",
    "                categories.append('PUC') \n",
    "            elif arg0 in ['you', 'your'] and verb in ['choose', 'are']:\n",
    "                categories.append('PUPR')\n",
    "            elif arg0 in ['we', 'us', 'our'] and verb in ['use', 'uses', 'perform', 'collect', 'allow', 'provides', 'gather']:\n",
    "                categories.append('POPR')\n",
    "            elif arg0 in ['we', 'us', 'our'] and verb in ['store', 'maintains']:\n",
    "                categories.append('POC')\n",
    "            \n",
    "            if arg0 == org and mod in ['must', 'will', 'would']:\n",
    "                categories.append('POC')\n",
    "            elif arg0 == org and mod in ['may','might','can','could']:\n",
    "                categories.append('POPR')\n",
    "            elif arg0 == org and verb in ['store', 'maintains']:\n",
    "                categories.append('POC')\n",
    "            elif arg0 == org and verb in ['use', 'uses', 'perform', 'collect', 'allow', 'provides', 'gather']:\n",
    "                categories.append('POPR')\n",
    "                \n",
    "    if categories:\n",
    "        return categories[0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAttributes(srl):\n",
    "    slrDic = literal_eval(srl)\n",
    "    if slrDic is not None:\n",
    "#         verbs = slrDic['verb']\n",
    "            attributes = {}\n",
    "            description = slrDic['description']\n",
    "            entities = re.findall(r'([a-zA-Z0-9- ]*:[a-zA-Z0-9\\- ]*)', description)\n",
    "            dic = dict(s.split(':') for s in entities)\n",
    "            \n",
    "            arg0 = \"\"\n",
    "            verb = \"\"\n",
    "            arg1 = \"\"\n",
    "            arg2 = \"\"\n",
    "            argm_prp = \"\"\n",
    "            argm_adv = \"\"\n",
    "            \n",
    "            if dic.get('ARG0') is not None:\n",
    "                arg0 = dic.get('ARG0').strip().lower()\n",
    "            if dic.get('V') is not None:\n",
    "                verb = dic.get('V').strip().lower()\n",
    "            if dic.get('ARG1') is not None:\n",
    "                arg1 = dic.get('ARG1').lower()\n",
    "            if dic.get('ARG2') is not None:\n",
    "                arg2 = dic.get('ARG2').lower()\n",
    "            if dic.get('ARGM-PRP') is not None:\n",
    "                argm_prp = dic.get('ARGM-PRP').lower()\n",
    "            if dic.get('ARGM-ADV') is not None:\n",
    "                argm_adv = dic.get('ARGM-ADV').lower()\n",
    "                \n",
    "            if arg0 != \"\":\n",
    "                attributes['actor'] = arg0\n",
    "                attributes['action'] = verb\n",
    "                attributes['object'] = arg1\n",
    "#                 with,to, for preposition for identifying the target\n",
    "#                check arg1 for object source\n",
    "                if re.findall(r'\\byour|you\\b', arg1):\n",
    "                    attributes['object_source'] = \"user(s)\"\n",
    "                elif re.findall(r'\\bour|us\\b', arg1):\n",
    "                    attributes['object_source'] = \"organization\"\n",
    "                else:\n",
    "                    attributes['object_source'] = \"\"\n",
    "                attributes['target'] = arg2     \n",
    "                attributes['purpose'] = argm_prp\n",
    "                attributes['conditions'] = argm_adv\n",
    "                return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrame(verb):\n",
    "    return framenet.frames(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''https://wordnet.princeton.edu/documentation/lexnames5wn'''\n",
    "def getLexName(verb):\n",
    "    synsets = wordnet.synsets(verb, pos=wordnet.VERB)\n",
    "    for synset in synsets:\n",
    "        return synset.lexname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def annotatedByDepandancy(depandancy):\n",
    "#     depandancyDic = literal_eval(depandancy)\n",
    "#     if 'children' in depandancyDic:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printStatistics(df):\n",
    "    \n",
    "#     print(\"===========Corpus Information===============\")\n",
    "#     print(\"Word count: {}\".format())\n",
    "    \n",
    "    print(\"===========Instance Count===============\")\n",
    "    instaceCount = df.groupby('annotated_class').count()['statement']\n",
    "    print(instaceCount)\n",
    "    print(\"Actor organization:{}\".format(instaceCount[0] + instaceCount[1]))\n",
    "    print(\"Actor user:{}\".format(instaceCount[2] + instaceCount[3]))\n",
    "    \n",
    "    srlClassed = df[df['slr_based_class'] != \"None\"]\n",
    "    print(\"===========Conusion Matrix===============\")\n",
    "    coverage = (srlClassed.shape[0]/100)*100\n",
    "    print(\"Coverage:{}%\".format(coverage))\n",
    "    \n",
    "\n",
    "    actual = srlClassed['annotated_class']\n",
    "    predicted = srlClassed['slr_based_class']\n",
    "    print(confusion_matrix(actual,predicted))\n",
    "    print(\"Accuray:{}\".format(accuracy_score(actual, predicted)))\n",
    "    print(classification_report(actual,predicted))\n",
    "    \n",
    "    print(\"===========Attribute details===============\")\n",
    "    print(df.groupby('action').count()['statement'].sort_values(ascending=False))\n",
    "    print(df.groupby('lex_name').count()['action'].sort_values(ascending=False))\n",
    "    \n",
    "    matchedActor = 0\n",
    "    matchedAction = 0\n",
    "    matchedObject = 0\n",
    "    matchedObjectSource = 0\n",
    "    matchedTarget = 0\n",
    "    matchedPurpose = 0\n",
    "    matchedCondition = 0\n",
    "    \n",
    "    replacedDf = df.replace(np.nan, '', regex=True)\n",
    "    for i, row in replacedDf.iterrows():\n",
    "        print(row['a_target'])\n",
    "        print(row['target'])\n",
    "        if row['a_actor'] != \"\" and row['actor'] == row['a_actor']:\n",
    "            matchedActor+=1\n",
    "        if row['a_action'] != \"\" and row['action'].strip() == row['a_action'].strip():\n",
    "            matchedAction+=1\n",
    "        if row['a_object'] != \"\" and row['a_object'] == row['object']:\n",
    "            matchedObject+=1\n",
    "        if row['a_object_source'] != \"\" and row['a_object_source'] == row['object_source']:\n",
    "            matchedObjectSource+=1\n",
    "        if row['a_target'] != \"\" and row['a_target'] == row['target']:\n",
    "            matchedTarget+=1\n",
    "        if row['a_purpose'] != \"\" and row['a_purpose'] == row['purpose']:\n",
    "            matchedPurpose+=1\n",
    "        if row['a_condition'] != \"\" and row['a_condition'] in row['conditions']:\n",
    "            matchedCondition+=1\n",
    "    \n",
    "    print(\"================Matched Attribute Count==================\")\n",
    "    print(\"Actor:{}\".format(matchedActor))\n",
    "    print(\"Action:{}\".format(matchedAction))\n",
    "    print(\"Object:{}\".format(matchedObject))\n",
    "    print(\"Object Source:{}\".format(matchedObjectSource))\n",
    "    print(\"Target:{}\".format(matchedTarget))\n",
    "    print(\"Purpose:{}\".format(matchedPurpose))\n",
    "    print(\"Condition:{}\".format(matchedCondition))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slr_based_class\n",
      "None    255\n",
      "POC       4\n",
      "POPR     39\n",
      "PUC      36\n",
      "PUPR     16\n",
      "Name: statement, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# dataFolderPath = \"/home/lahiru/Research/policy_analysis/data/usableprivacy/OptOutChoice-2017_v1.0/SentenceDict.json\"\n",
    "# segmentDf = parsedPolicySegments(dataFolderPath).sample(n=100)\n",
    "# cleanSegments = cleanSegments(segmentDf)\n",
    "# filtredSegments =cleanSegments[cleanSegments['statement'].str.strip() != \"\"].sample(n=1000)\n",
    "# segmentDf.to_csv('processed_privacy_policy_segments_sample_100_4.csv',',')\n",
    "\n",
    "# extractSLR(cleanSegments)\n",
    "# extractedSegments = extractActor(cleanSegments)\n",
    "# extractedSegments.to_csv('processed_privacy_policy_segments.csv',',')\n",
    "\n",
    "# segmentDf = pd.read_csv('processed_privacy_policy_segments_sample_2000_1.csv')\n",
    "# extractedSegments = extractor(segmentDf)\n",
    "# extractedSegments.to_csv('processed_privacy_policy_segments_sample_2000_1.csv',',')\n",
    "\n",
    "# annotatedStatement = pd.read_csv('annotated_privacy_policy_segments_100_nlp_pipeline.csv')\n",
    "# extractedSegments = extractor(annotatedStatement)\n",
    "# extractedSegments.to_csv('annotated_privacy_policy_segments_100_nlp_pipeline.csv',',')\n",
    "# printStatistics(annotatedStatement)\n",
    "\n",
    "# tokenizedSatatements = preProcessedSegments(cleanSegments)\n",
    "# trainWordEmbedding(tokenizedSatatements)\n",
    "# loadFastTextModel('word_embedding_model/word2vec_policy_modal.model')\n",
    "# loadGoogleDataModel('word_embedding_model/GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "# tokenizedAppPolicy = parsedAppPolicyData('/home/lahiru/Research/policy_analysis/src/scripts/crawl_privacy_page/app_privacy/')\n",
    "# trainWordEmbedding(tokenizedAppPolicy)\n",
    "# loadFastTextModel('word_embedding_model/word2vec_app_policy_modal.model')\n",
    "# segmentDf = pd.read_csv('processed_privacy_policy_segments_sample_2000_1.csv')\n",
    "# dfClass = segmentDf[(segmentDf['slr_based_class'] != 'None') & (segmentDf['slr_based_class'] != null)]\n",
    "# dfClass.to_csv('processed_privacy_policy_segments_sample_2000_1_class.csv',',')\n",
    "\n",
    "# dfNone = segmentDf[(segmentDf['slr_based_class'] == 'None') | (segmentDf['slr_based_class'] == ' ')]\n",
    "# dfNone.to_csv('processed_privacy_policy_segments_sample_2000_1_None.csv',',')\n",
    "\n",
    "new_data = [\"We use cookies on this Site to ensure the integrity of the registration process and to personalize the Site.\",\n",
    "            \"Our Sites' registration system requires users to give us various Personal Data, such as their name and e-mail address, ZIP code, sex, age or income level\",\n",
    "            \"We may change the terms of this Notice at any time.\",\n",
    "            \"Generally, you can access and browse our Web sites without disclosing any personally identifiable information.\",\n",
    "            \"E-mail Alert Service Registration Form: If you complete the E- mail Alert Service Registration Form, you will receive e-mail alerts regarding new press releases posted to the Web Site(s).\",\n",
    "           \"From time to time, we may request personal information from you at our sites in order to deliver requested materials to you.\"]\n",
    "\n",
    "# df = pd.DataFrame(new_data, columns = ['statement'])\n",
    "\n",
    "# df = pd.read_csv('processed_privacy_policy_segments_paper.csv')\n",
    "# extractedSegments = extractor(df)\n",
    "# extractedSegments.to_csv('processed_privacy_policy_segments_paper.csv',',')\n",
    "\n",
    "#=====================================Diversify experiment==============================================\n",
    "# dataFolderPath = \"/home/lahiru/Research/policy_analysis/data/usableprivacy/OptOutChoice-2017_v1.0/SentenceDict.json\"\n",
    "# segmentDf = parsedPolicySegments(dataFolderPath)\n",
    "# newsWebsites = [481,862,1089,1360,1361,1637,1683]\n",
    "# healthWebsites = [202,517,581,642,884,891,898,1221]\n",
    "# shoppingWebsites = [640,807,1498]\n",
    "\n",
    "# newsWebsiteDf = selectPolicySentences(segmentDf, newsWebsites)\n",
    "# print(newsWebsiteDf)\n",
    "# newsWebsiteDf.to_csv('selected_website_analysis/privacy_policy_segments_news_websites.csv',',')\n",
    "\n",
    "# healthWebsiteDf = selectPolicySentences(segmentDf, healthWebsites)\n",
    "# healthWebsiteDf.to_csv('selected_website_analysis/privacy_policy_health_websites.csv',',')\n",
    "\n",
    "# shoppingWebsiteDf = selectPolicySentences(segmentDf, shoppingWebsites)\n",
    "# shoppingWebsiteDf.to_csv('selected_website_analysis/privacy_policy_shopping_websites.csv',',')\n",
    "\n",
    "# annotatedNewsPolicy = pd.read_csv('selected_website_analysis/privacy_policy_segments_news_websites_nlp_pipeline.csv')\n",
    "# extractedNewsPolicy = extractor(annotatedNewsPolicy)\n",
    "# extractedNewsPolicy.to_csv('selected_website_analysis/privacy_policy_segments_news_websites_nlp_pipeline.csv',',')\n",
    "\n",
    "# annotatedHealthPolicy = pd.read_csv('selected_website_analysis/privacy_policy_segments_health_websites_nlp_pipeline.csv')\n",
    "# extractedHealthPolicy = extractor(annotatedHealthPolicy)\n",
    "# extractedHealthPolicy.to_csv('selected_website_analysis/privacy_policy_segments_health_websites_nlp_pipeline.csv',',')\n",
    "\n",
    "# annotatedShoppingPolicy = pd.read_csv('selected_website_analysis/privacy_policy_segments_shopping_websites_nlp_pipeline.csv')\n",
    "# extractedShoppingPolicy = extractor(annotatedShoppingPolicy)\n",
    "# extractedShoppingPolicy.to_csv('selected_website_analysis/privacy_policy_segments_shopping_websites_nlp_pipeline.csv',',')\n",
    "\n",
    "# resultNewsPolicy = annotatedNewsPolicy.groupby('slr_based_class').count()['statement']\n",
    "# print(resultNewsPolicy)\n",
    "\n",
    "# resultHelathPolicy = annotatedHealthPolicy.groupby('slr_based_class').count()['statement']\n",
    "# print(resultHelathPolicy)\n",
    "\n",
    "# resultShoppingPolicy = annotatedShoppingPolicy.groupby('slr_based_class').count()['statement']\n",
    "# print(resultShoppingPolicy)\n",
    "\n",
    "#=====================================Specific website==============================================\n",
    "# dataFolderPath = \"/home/lahiru/Research/policy_analysis/data/usableprivacy/OptOutChoice-2017_v1.0/SentenceDict.json\"\n",
    "# segmentDf = parsedPolicySegments(dataFolderPath)\n",
    "\n",
    "# everyDayHealthDF = selectPolicySentences(segmentDf, [891])\n",
    "# print(everyDayHealthDF)\n",
    "# everyDayHealthDF.to_csv('selected_website_analysis/privacy_policy_segments_everydayhealth_website.csv',',')\n",
    "\n",
    "annotatedEveryDayHealthPolicy = pd.read_csv('selected_website_analysis/privacy_policy_segments_everydayhealth_website_nlp_pipeline.csv')\n",
    "# extractedEveryDayHealthPolicy = extractor(annotatedEveryDayHealthPolicy)\n",
    "# extractedEveryDayHealthPolicy.to_csv('selected_website_analysis/privacy_policy_segments_everydayhealth_website_nlp_pipeline.csv',',')\n",
    "\n",
    "resultNewsPolicy = annotatedEveryDayHealthPolicy.groupby('slr_based_class').count()['statement']\n",
    "print(resultNewsPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
